---
title: "STA-380-Homework-1-Dalton-Haines-Jansen-Karnes"
output: html_document
---
Contributors: Reed Dalton, Corey Haines, Alex Jansen, McKenna Karnes
```{r include = FALSE}
library(fImport)
library(mosaic)
library(foreach)
library(quantmod)
library(cluster)
library(ggplot2)
library(flexclust)
library(dplyr)
library(ggplot2)
options("getSymbols.warning4.0"=FALSE)
options("getSymbols.yahoo.warning"=FALSE)
set.seed(1)

```



#Probability Practice

### Part A. 
Given a single survey question to visitors to a particular website, what fraction of people who truthfully answered the question answered yes?      
* There are two types of clickers: Random Clickers (RC), and Truthful Clickers (TC)     
* There are two answers to the survey: yes and no   
* The expected fraction of Random Clickers (RC) is 0.3     
* The expected fraction of Truthful Clickers (TC) is 0.7     
* The probability that the Random Clickers (RC) selects yes is .5   
* The probability that the Random Clickers (RC) selects no is .5      
The results of the survey were such that 65% said Yes and 35% said No.          
```{r}
prob.yes = 0.65
prob.no = 0.35
prob.yes.given.RC = 0.5
prob.no.given.RC = 0.5
prob.TC = 0.7
prob.RC = 0.3
prob.yes.given.TC = (prob.yes - (prob.RC * prob.yes.given.RC))/prob.TC

```
`prob.yes` = `prob.TC` \* `prob.yes.given.TC` + `prob.RC` \* `prob.yes.given.RC`

The percentage of Truthful Clickers who answered "Yes" to the the simple survey is `r prob.yes.given.TC`

### Part B.
Medical test for disease using Bayes Formula. 
* The sensitivity is about 0.993. That is, if someone has the disease, there is a probability of 0.993 that they will test positive.     
* The specificity is about 0.9999. This means that if someone doesn't have the disease, there is probability of 0.9999 that they will test negative.     
* About 0.0025% of all people in the general population have the disease
If someone tests positive for the disease, what is the probability they actually have the disease?

```{r}
pop.infected = 0.000025
pop.clean = 0.999975

infected.pos = 0.993
infected.neg = 0.007

clean.pos = 0.0001
clean.neg = 0.9999
infected.test.pos = (infected.pos*pop.infected)/(infected.pos*pop.infected + clean.pos*pop.clean)
infected.test.pos = infected.test.pos * 100

```
If someone tested positive for the disease, there is only a `r infected.test.pos`% chance that they actually have the disease.  
 
Given our calculations, implementing a universal testing policy for this disease would be problematic because over 80% of the time a test for the disease is positive, the person will not actually be infected. This could lead to mass hysteria and put unneeded strain on the population's health systems.       

#Exploratory Analysis: Green Buildings     
Our approach to this problem was to narrow down the dataset of apartments given to a subset more similar to the potential Austin apartment. We could then compare the cost of green to nongreen buildings in each cluster in order to determine if there is a premium on green building rent across the board.

After determining this, we were able to make a suggestion to the real-estate developer on whether it would be profitable to build a green building.  If the premium on green rent was higher than the premium on green construction costs, then we decide she should build green.


```{r}
#setwd('/Users/Reeddalton/Documents/GitHub/STA-380-Homework-1-Dalton-Haines-Jansen-Karnes')
gb = read.csv('greenbuildings.csv')

rent.days=plot_ly(data=gb,x=~total_dd_07,y=~Rent)
```
`r rent.days`
When determining which variables to use to narrow down our dataset to create a target subset similar to the building to be constructed, we considered including values related to utilities (total_dd_07,Gas_Costs, Electricity_Costs, net),but we found that a large total number of heating and cooling days actually did not seem to be related to a large rent. Given this information, taking gas and electricity costs into account no longer intuitively felt useful in evaluating the Austin apartment's rent. We also found it nearly impossible to distinguish a rent change in buildings that include utilities in rent, as we do not know the utility cost nor if the Austin building will include them.

```{r}
p = plot_ly(data=gb,x=~age, y=~stories)
```
`r p`
We decided to cut age off at 50 and stories at 30 to capture a subset more similar to the building the real-estate developer is proposing (will be 0 years old and 15 stories)

```{r}
gb = gb[gb$age <=50,]
gb = gb[gb$stories <=30,]
q = plot_ly(data=gb,x=~age, y=~stories)
```
`r q`
Our new plot showing the filtered data set

```{r}
#filtering out green and non green buildings
greenbuildings = gb[gb$green_rating == 1,]  
nongreen = gb[gb$green_rating==0,]
#Range of green rents and non green rents
green.rent = plot_ly(data=greenbuildings,x=~Rent)
non.green.rent = plot_ly(data=nongreen, x=~Rent)

```
Both bar charts have somewhat wide ranges with most of the data being in between 0 and 80. Because of this, the on-staff statistician's method of separating and merely taking averages wasn't prudent. We need to be comparing green and non green within clusters with similar characteristics to our building.

```{r}
#Set up vectors to hold the ratios
greenratios = c()
nongreenratios = c()
greenratios = greenbuildings$Rent/greenbuildings$cluster_rent
nongreenratios = nongreen$Rent/nongreen$cluster_rent

#average
average.green = mean(greenratios)
average.nongreen = mean(nongreenratios)

#averages on averages
quotientavg = average.green/average.nongreen
```
`r mean(quotientavg)`

We determined that the premium on green rent was 6.75%. This is larger than the 5% premium on green construction in the Austin area, so we will advise the real-estate developer to proceed with building green.






#Bootstrapping and ETF Returns
* To understand the tradeoffs of risk and return of Exchange Traded Funds, we explored the growth over several years of historical data of the following ETFs:
    + US domestic equities (SPY: the S&P 500 stock index)      
    + US Treasury bonds (TLT)      
    + Investment-grade corporate bonds (LQD)     
    + Emerging-market equities (EEM)      
    + Real estate (VNQ)     
* We considered three portfolios with varying levels of risk:
    +Even Split: 20% of  assets in each of the five ETFs 
    + Safer Split: investments  assets in at least three classes      
    + Aggresive Split: consolidating assets into fewer classes with the promise of higher returns at the cost of accepting greater risk   
* We have assumed the portfolio is rebalanced for free each day i.e. with zero transaction costs
```{r}


#Import the stocks
mystocks = c("SPY", "TLT", "LQD", "EEM", "VNQ")
getSymbols(mystocks)

#Adjust for splits and dividends
SPYa = adjustOHLC(SPY)
TLTa = adjustOHLC(TLT)
LQDa = adjustOHLC(LQD)
EEMa = adjustOHLC(EEM)
VNQa = adjustOHLC(VNQ)

```
#Marshals appropriate evidence
I think we need to do some sort of visualization here to show why bonds and the s&p500 are 'safe' investments, while real estate and emerging markets are 'risky' investments.
```{r}
SPYa.adj <- SPYa[,6]
TLTa.adj <- TLTa[,6]
LQDa.adj <- LQDa[,6]
EEMa.adj <- EEMa[,6]
VNQa.adj <- VNQa[,6]

SPYa1 = as.numeric(SPYa.adj[1])
TLTa1 = as.numeric(TLTa.adj[1])
LQDa1 = as.numeric(LQDa.adj[1])
EEMa1 = as.numeric(EEMa.adj[1])
VNQa1 = as.numeric(VNQa.adj[1])

SPYa.ts =SPYa.adj/SPYa1
TLTa.ts =TLTa.adj/TLTa1
LQDa.ts =LQDa.adj/LQDa1
EEMa.ts =EEMa.adj/EEMa1
VNQa.ts =VNQa.adj/VNQa1

basket <- cbind(SPYa.ts,TLTa.ts,LQDa.ts,EEMa.ts,VNQa.ts  )
zoo.basket <- as.zoo(basket)
tsRainbow <- rainbow(ncol(zoo.basket))
plot(x = zoo.basket, ylab = "Cumulative Return", main = "Cumulative Returns",col = tsRainbow, screens = 1)
legend(x = "topleft", legend = c("SPY", "TLT", "LQD", "EEM", "VNQ"), lty = 1,col = tsRainbow)



```





###Even Split Portfolio

```{r}
#############################
#########Even Split Portfolio
#############################

#Setting up an all returns matrix showing closing prices for each day 
all_returns = cbind(ClCl(SPYa),ClCl(TLTa),ClCl(LQDa), ClCl(EEMa), ClCl(VNQa))
all_returns = as.matrix(na.omit(all_returns))

n_days = 20#i.e. four weeks

#Setting up the Even Split Portfolio and using Bootstrap to find VaR
initial.wealth = 100000
sim.even = foreach(i=1:5000, .combine = 'rbind')%do%{
  days = 20
  total.wealth = initial.wealth
  even.weight = c(0.2,0.2,0.2,0.2,0.2)
  even.holdings = even.weight*total.wealth
  even.wealthtracker = rep(0,days)
  #loop over four trading weeks
  for(today in 1:days){
    return.today = resample(all_returns, 1, orig.ids = FALSE)
    even.holdings = even.holdings + even.holdings*return.today
    total.wealth = sum(even.holdings)
    even.wealthtracker[today] = total.wealth
  }
  even.wealthtracker
}

#Calculate 5% VaR for even portfolio
even.split.VaR = quantile(sim.even[,n_days], 0.05) - initial.wealth
print(sprintf('The value at risk at the five percent level for our even split portfolio is %f: ', even.split.VaR))
```


###Safer Split
```{r}
##################
###### Safer Split
##################

safe_returns = cbind(ClCl(SPYa),ClCl(TLTa),ClCl(LQDa))
safe_returns = as.matrix(na.omit(safe_returns))

#Setting up the Safe Split Portfolio and using Bootstrap to find VaR

initial.wealth = 100000

sim.safe = foreach(i=1:5000, .combine = 'rbind')%do%{
  days = 20
  total.wealth = initial.wealth
  safe.weight = c(1/3,1/3,1/3)
  safe.holdings = safe.weight*total.wealth
  safe.wealthtracker = rep(0,days)
  #loop over four trading weeks
  for(today in 1:days){
    return.today = resample(safe_returns, 1, orig.ids = FALSE)
    safe.holdings = safe.holdings + safe.holdings*return.today
    total.wealth = sum(safe.holdings)
    safe.wealthtracker[today] = total.wealth
  }
  safe.wealthtracker
}

#Calculate 5% VaR for safe portfolio
safe.split.VaR = quantile(sim.safe[,n_days], 0.05) - initial.wealth
print(sprintf('The value at risk at the five percent level for our safe split portfolio is %f: ', safe.split.VaR))
```


##Aggressive Portfolio
```{r}
########################
####Aggressive Portfolio
########################
aggressive_returns = cbind(ClCl(EEMa),ClCl(VNQa))
aggressive_returns = as.matrix(na.omit(aggressive_returns))

#Setting up the Agressive Split Portfolio and using Bootstrap to find VaR

initial.wealth = 100000

sim.aggressive = foreach(i=1:5000, .combine = 'rbind')%do%{
  days = 20
  total.wealth = initial.wealth
  aggressive.weight = c(1/2,1/2)
  aggressive.holdings = aggressive.weight*total.wealth
  aggressive.wealthtracker = rep(0,days)
  #loop over four trading weeks
  for(today in 1:days){
    return.today = resample(aggressive_returns, 1, orig.ids = FALSE)
    aggressive.holdings = aggressive.holdings + aggressive.holdings*return.today
    total.wealth = sum(aggressive.holdings)
    aggressive.wealthtracker[today] = total.wealth
  }
  aggressive.wealthtracker
}

#Calculate 5% VaR for aggressive portfolio
aggressive.split.VaR = quantile(sim.aggressive[,n_days], 0.05) - initial.wealth
print(sprintf('The value at risk at the five percent level for our aggressive split portfolio is %f: ', aggressive.split.VaR))

```


#Market Segmentation and NutrientH2O
```{r}
social_marketing = read.csv('social_marketing.csv')

sm <- social_marketing[,-1]/rowSums(social_marketing[,-1], na.rm = FALSE)
sm = cbind(X = social_marketing[,1], sm)

social_marketing$Total = rowSums(social_marketing[,-1], na.rm = FALSE)

total_tweets_by_category = colSums(social_marketing[,-1], na.rm = FALSE)
sort(total_tweets_by_category)
#pairs(social_marketing[,-1])
```
I created a matrix where I found the proportion that each user tweeted about each category. This was done in an attempt to standardize across the distribution of activity for users. 

I then added the total number of tweets per user to the last column of social_marketing. I then found the total number of tweets for each of the 36 different categories.  

From inspecting the sm dataframe, I am not as concerned about tweets categorized as "spam". No account tweets out more than 2 tweets categorized as spam, and the highest percentage of an account's tweets being categorized as spam is 7%.   

However, "adult" is a little less clear, as there are 2 accounts where half of the tweets are adult. These are probably just horny old men who don't understand how to use twitter and will still consume NutrientH2O, so I think we can leave them in. 

```{r}

apply(social_marketing[,-1] , 2, mean)
pr.out =prcomp(social_marketing[,-1] , scale =TRUE)
#biplot (pr.out , scale =0)
pr.out$rotation
pr.var =pr.out$sdev^2
pve=pr.var/sum(pr.var)
plot(pve , xlab=" Principal Component ", ylab=" Proportion of
Variance Explained ", ylim=c(0,1) ,type='b')
plot(cumsum (pve), xlab=" Principal Component ", ylab ="
Cumulative Proportion of Variance Explained ", ylim=c(0,1) ,
type='b')
```

```{r}
set.seed (3)
km.out =kmeans(social_marketing[,-1],3, nstart =50)
km.out$tot.withinss

#plot(social_marketing[,-1], col =(km.out$cluster +1) , main="K-Means Clustering Results with K=3", pch =20, cex =2)


```

```{r}

data(social_marketing[,-1])
km    <- kmeans(social_marketing[,-1],3)
dissE <- daisy(social_marketing[,-1]) 
dE2   <- dissE^2
sk2   <- silhouette(km$cl, dE2)
plot(sk2)


```


